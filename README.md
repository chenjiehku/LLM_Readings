# Reading materials for Large Language Model (LLM)
This repo is created to manage the reading materials about LLM.

## Model Architecture
1. [Attention Is All You Need (Transformers)](https://arxiv.org/pdf/1706.03762.pdf)
2. [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/)
3. [The Transformer Family](https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/)
4. [The Transformer Family Version 2.0](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)
5. [Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)
6. [Improving Language Understanding by Generative Pre-Training (GPT 1.0)](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)
7. [Language Models are Unsupervised Multitask Learners (GPT 2.0)](https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf)
8. [Language Models are Few-Shot Learners (GPT 3.0)](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)
9. [GPT in 60 Lines of NumPy](https://jaykmody.com/blog/gpt-from-scratch/)
10. [X-Transformers](https://github.com/lucidrains/x-transformers)

## Inference Optimization
1. [Large Transformer Model Inference Optimization](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)
2. [Transformer Inference Arithmetic](https://kipp.ly/blog/transformer-inference-arithmetic/)
3. [Full Stack Optimization of Transformer Inference: a Survey](https://arxiv.org/pdf/2302.14017.pdf)

## Training Recipe
1. [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/pdf/1909.08593.pdf)
2. [Training language models to follow instructions with human feedback (InstructGPT)](https://arxiv.org/pdf/2203.02155.pdf)
3. [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)
4. [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/pdf/2212.08073.pdf)
5. [Pythia-A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/pdf/2304.01373.pdf)
6. [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)
7. [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068.pdf)
8. [Stanford Alpaca: An Instruction-following LLaMA Model](https://github.com/tatsu-lab/stanford_alpaca)
9. [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)
10. [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf)
11. [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf) 

## Training Dataset
1. [LLMDataHub: Awesome Datasets for LLM Training](https://github.com/Zjh-819/LLMDataHub)

## Training Framework
1. [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
2. [DeepSpeed](https://github.com/microsoft/DeepSpeed)
3. [VeOmni](https://github.com/ByteDance-Seed/VeOmni)
4. [verl](https://github.com/volcengine/verl)

## LLM for Coding
1. [Evaluating Large Language Models Trained on Code (Codex)](https://arxiv.org/pdf/2107.03374.pdf)
2. [CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis](https://arxiv.org/pdf/2203.13474.pdf?trk=public_post_comment-text)
3. [CodeGen2: Lessons for Training LLMs on Programming and Natural Languages](https://arxiv.org/pdf/2305.02309.pdf)
4. [StarCoder: may the source be with you!](https://arxiv.org/pdf/2305.06161.pdf)
5. [A Systematic Evaluation of Large Language Models of Code](https://arxiv.org/pdf/2202.13169.pdf)

## RLHF for LLM
1. [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)
2. [StackLLaMA: A hands-on guide to train LLaMA with RLHF](https://huggingface.co/blog/stackllama)

## LLM Evaluation
1. [Holistic Evaluation of Language Models](https://arxiv.org/pdf/2211.09110.pdf)
2. [HumanEval: Hand-Written Evaluation Set](https://github.com/openai/human-eval/tree/master)
3. [Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs](https://arxiv.org/pdf/2305.03111.pdf)

## DeepSeek Series
1. [DeepSeek Github](https://github.com/deepseek-ai)
2. 2024.01.05: [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](https://arxiv.org/abs/2401.02954)
3. 2024.01.25: [DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence](https://arxiv.org/abs/2401.14196)
4. 2024.02.05: [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300)
5. 2024.03.08: [DeepSeek-VL: Towards Real-World Vision-Language Understanding](https://arxiv.org/abs/2403.05525)
6. 2024.05.07: [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434)
7. 2024.06.17: [DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://arxiv.org/abs/2406.11931)
8. 2024.12.13: [DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding](https://arxiv.org/abs/2412.10302)
9. 2024.12.27: [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437)
10. 2025.01.22: [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)

## Qwen Series
1. [Qwen Team Blog](https://qwenlm.github.io/blog/)
2. [Qwen Team Github](https://github.com/QwenLM)
3. 2023.08.24: [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://arxiv.org/abs/2308.12966)
4. 2023.09.28: [Qwen Technical Report](https://arxiv.org/abs/2309.16609)
5. 2023.11.14: [Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models](https://arxiv.org/abs/2311.07919)
6. 2024.07.15: [Qwen2 Technical Report](https://arxiv.org/abs/2407.10671)
7. 2024.07.15: [Qwen2-Audio Technical Report](https://arxiv.org/abs/2407.10759)
8. 2024.09.18: [Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution](https://arxiv.org/abs/2409.12191)
9. 2024.09.18: [Qwen2.5-Coder Technical Report](https://arxiv.org/abs/2409.12186)
10. 2024.09.18: [Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement](https://arxiv.org/abs/2409.12122)
11. 2024.12.19: [Qwen2.5 Technical Report](https://arxiv.org/abs/2412.15115)
12. 2025.02.19: [Qwen2.5-VL Technical Report](https://arxiv.org/abs/2502.13923)
13. 2025.03.26: [Qwen2.5-Omni Technical Report](https://arxiv.org/abs/2503.20215)

## Open Source R1 SFT Dataset
1. [open-r1/OpenR1-Math-220k](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k)
2. [open-r1/codeforces-cots](https://huggingface.co/datasets/open-r1/codeforces-cots)
3. [open-thoughts/OpenThoughts-114k](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k)
4. [open-thoughts/OpenThoughts2-1M](https://huggingface.co/datasets/open-thoughts/OpenThoughts2-1M)
5. [Congliu/Chinese-DeepSeek-R1-Distill-data-110k](https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k)
6. [a-m-team/AM-DeepSeek-R1-Distilled-1.4M](https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M)
7. [qihoo360/Light-R1-SFTData](https://huggingface.co/datasets/qihoo360/Light-R1-SFTData)
8. [simplescaling/s1K-1.1](https://huggingface.co/datasets/simplescaling/s1K-1.1)
9. [nvidia/OpenCodeReasoning](https://huggingface.co/datasets/nvidia/OpenCodeReasoning)

## Open Source R1 RL Dataset
1. [open-r1/codeforces](https://huggingface.co/datasets/open-r1/codeforces)
2. [open-r1/DAPO-Math-17k-Processed](https://huggingface.co/datasets/open-r1/DAPO-Math-17k-Processed)
