# Reading materials for Large Language Model (LLM)
This repo is created to manage the reading materials about LLM.

## Model Architecture
1. [Attention Is All You Need (Transformers)](https://arxiv.org/pdf/1706.03762.pdf)
2. [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/)
3. [The Transformer Family](https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/)
4. [The Transformer Family Version 2.0](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)
5. [Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)
6. [Improving Language Understanding by Generative Pre-Training (GPT 1.0)](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)
7. [Language Models are Unsupervised Multitask Learners (GPT 2.0)](https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf)
8. [Language Models are Few-Shot Learners (GPT 3.0)](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)
9. [GPT in 60 Lines of NumPy](https://jaykmody.com/blog/gpt-from-scratch/)
10. [X-Transformers](https://github.com/lucidrains/x-transformers)

## Inference Optimization
1. [Large Transformer Model Inference Optimization](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)
2. [Transformer Inference Arithmetic](https://kipp.ly/blog/transformer-inference-arithmetic/)
3. [Full Stack Optimization of Transformer Inference: a Survey](https://arxiv.org/pdf/2302.14017.pdf)

## Training Recipe
1. [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/pdf/1909.08593.pdf)
2. [Training language models to follow instructions with human feedback (InstructGPT)](https://arxiv.org/pdf/2203.02155.pdf)
3. [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)
4. [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/pdf/2212.08073.pdf)
5. [Pythia-A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/pdf/2304.01373.pdf)
6. [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)
7. [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068.pdf)
8. [Stanford Alpaca: An Instruction-following LLaMA Model](https://github.com/tatsu-lab/stanford_alpaca)
9. [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)
10. [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf)
11. [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf) 

## Training Dataset
1. [LLMDataHub: Awesome Datasets for LLM Training](https://github.com/Zjh-819/LLMDataHub)

## Training Framework
1. [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
2. [DeepSpeed](https://github.com/microsoft/DeepSpeed)
3. [GPT-NeoX](https://github.com/EleutherAI/gpt-neox)
4. [TRL-Transformer Reinforcement Learning](https://github.com/lvwerra/trl)
5. [TRLX-Transformer Reinforcement Learning X](https://github.com/CarperAI/trlx/tree/main)

## LLM for Coding
1. [Evaluating Large Language Models Trained on Code (Codex)](https://arxiv.org/pdf/2107.03374.pdf)
2. [CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis](https://arxiv.org/pdf/2203.13474.pdf?trk=public_post_comment-text)
3. [CodeGen2: Lessons for Training LLMs on Programming and Natural Languages](https://arxiv.org/pdf/2305.02309.pdf)
4. [StarCoder: may the source be with you!](https://arxiv.org/pdf/2305.06161.pdf)
5. [A Systematic Evaluation of Large Language Models of Code](https://arxiv.org/pdf/2202.13169.pdf)

## RLHF for LLM
1. [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)
2. [StackLLaMA: A hands-on guide to train LLaMA with RLHF](https://huggingface.co/blog/stackllama)

## LLM Evaluation
1. [Holistic Evaluation of Language Models](https://arxiv.org/pdf/2211.09110.pdf)
2. [HumanEval: Hand-Written Evaluation Set](https://github.com/openai/human-eval/tree/master)
3. [Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs](https://arxiv.org/pdf/2305.03111.pdf)
